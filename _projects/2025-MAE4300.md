---
layout: project
title: Ethical Analysis of the Boeing 747 MAX Failures
description: Deep dive into the motives and structures which allowed these failures to occur
technologies: [Ethics]
image: /assets/images/MAE 4300.jpg
---

The investigation into the 737 MAX crashes showed how a series of technical decisions, certification practices, and organizational pressures enabled a preventable tragedy. The MCAS system, responsible for determining and adjusting the plane's heading, was the key technical failure point. Functionally, it lacked redundancy or adequate alerts. By relying on a single angle of attack sensor, any faulty data could trigger uncontrollable pitching. Pilots were not informed about the system, and important cues like AOA disagree warnings were either absent or optional. Regulators relied on Boeing’s safety assessments and treated MCAS as a minor change, which limited the scope of human factors evaluations and training requirements. When combined, these details created conditions where a technical fault could escalate into a loss of control, and the system behaved in ways that pilots could not reasonably anticipate. 

Looking at the broader context, each fact ties back to incentives acting on individuals and organizations. Boeing engineers operated under strong schedule and competitive pressures, especially in response to Airbus’s A320neo, and internal communication indicates that concerns about MCAS behavior were softened, delayed, or never elevated. Managers prioritized avoiding a new pilot type rating because it carried financial and market penalties. FAA regulators, working within an outdated risk assessment framework, delegated significant certification authority to Boeing and often lacked complete information about changes to MCAS. Investors, concerned with profitability and growth, indirectly reinforced the incentive to move quickly and minimize public attention to design changes. These pressures together shaped how information was presented, how risks were interpreted, and how engineering judgment was constrained. 

The ethical issues reflect different interpretations of key terms like safety, transparency, risk, and accountability. Boeing tended to frame safety as meeting established internal and regulatory thresholds. Families of victims saw safety as eliminating foreseeable harm. Regulators saw transparency as receiving the necessary documents, while the public expected clear communication about any safety-critical system. When these definitions diverge, ethical judgments diverge as well. Engineers may follow established norms while still contributing to outcomes that violate their professional responsibility to protect the public. Similar patterns appear in the materials on AI for predictive maintenance, where overconfidence in technology, loss of human expertise, and opaque decision pathways create new risks if organizational structures do not adapt. 

Preventing similar failures requires adjustments at every level. Engineers need clear avenues to raise concerns without fear of professional or organizational consequences, as well as training that reinforces independent judgment when safety is at stake. Organizations must update risk frameworks, strengthen review processes for system changes, and ensure that safety features cannot be classified as minor when they alter aircraft handling. Regulators need modernized procedures, reduced reliance on delegated authority, and stronger capabilities for independent analysis. System-wide, the incentive structure must reward thoroughness instead of speed by aligning investor expectations, certification timelines, and engineering norms around long-term reliability. Together these steps support a system where innovation can continue without allowing preventable hazards to go unchallenged.

